{#forecasting}
## Forecasting

"How long is it going to take?"

We get asked this question often.
So very often.
And the truth is, hardly anybody wants to really know how long a story or feature is going to take.
What they want to know is when it will be done.
And perhaps, more accurately, they want to know when they can have it or perhaps when they will be able to get the value it promises.

The most common technique I see for forecasting is good old velocity math.
Add up the work required to deliver the feature and divide it by the team's velocity.
This gives you the number of iterations it will take to deliver the feature.
Now multiply the number of iterations by the iteration length to determine how long it will take.

Let's say, for example, that our velocity for the last three iterations is 8, 16, and 15, for an average of 13.
Each iteration is two weeks.
Let's say the feature we are targeting is 84 points and there are an additional 24 points worth of work prioritized higher than some of the stories that comprise the feature.
In other words, in order to deliver the feature, we need to complete a total of 108 points, even though the feature itself is only 84 points.

~~~~~~~~
108 / 13 = 8.3 (round to 9)
2 * 9 = 18
~~~~~~~~

According to our typical "forecasting" technique, this work will be done in 18 weeks.

Cool.

How probable is this forecast?
As we discussed in the section on [Lead Time Distributions](#lead-time-distribution), the probability of the typical velocity math is pretty low - somewhere in the 50% range.
A coin toss, more or less.

Given enough lead time data, we could use the lead time distribution to help with forecasting, but there is an even better way.

Fortunately for us, Troy Magennis and the team at Focused Objective have done a great deal of work in this area and provide us a simple tool that allows us to forecast[^Forecaster] using a Monte Carlo simulation to produce a probability distribution.

That's a mouthful.

Essentially, their forecaster takes some basic inputs, such as throughput history, amount of work remaining, and average story split rate[^split-rate] and it runs 500 simulations to calculate the probability of the work being completed by a date.
The simulations are run by randomly selecting from the variables provided.
For each of the 500 simulations, an amount of work remaining is randomly selected from the range provided and then iterations are run by randomly applying a throughput and growth rate, based again on the ranges provided.

As you can see, this isn't running a basic burn down.
This is an actual simulation based on the numbers provided using actual data drawn from your project's history.

The 500 resulting completion dates are graphed on a distribution chart, allowing us to determine the probability of each potential completion date.

### Average

So let's take our current scenario and let's see how we do using our forecaster.
When we start with the average velocity of 13, 0 split rate, and 108 points to complete, all 500 simulations complete on the exact same date as our velocity math - 18 weeks from now.

![Forecast with Average](images/ForecastSettings.001.png)

![Average Forecast Results](images/ForecastResults.001.png)

This makes sense as there is no variance in the numbers provided.
Therefore the same backlog size is always chosen, the same throughput is applied to each iteration, and there is no growth or split rate.

But this just isn't realistic.
And it certainly isn't representative of our real world experiences.
Throughput / Velocity usually varies a bit from iteration to iteration.
Things happen; people are out, stories get blocked, we stay late to get more work done...

### Range
So let's run it again, but this time, we will use our velocity range instead of the average.
Using our prior three iterations, the velocity had a low of 8, and a high of 16.
We'll stick with a 0 split rate for this run.

![Forecast with Range](images/ForecastSettings.002.png)

![Range Forecast Results](images/ForecastResults.002.png)

This time, our 18 week projection is showing a 10 to 50% probability.
20 weeks has a 55 to 90% probability and 22 weeks and 24 weeks are 95% and 100% probable, respectively.

This confirms our already growing suspicion; the standard velocity burn down technique we've been using is less reliable than a coin toss.

### History

Let's run it again, but this time, let's use some actual throughput history rather than an average or a range.
To do this, we provide out throughput history for the past several iterations.

![Forecast Data](images/ForecastHistory.004.png)

We then adjust the settings to use data, rather than an estimate.

![Forecast with Data](images/ForecastSettings.004.png)

You might notice that, this time, the settings has some additional data in the lower right-hand corner.
This information gives us an indication of our throughput stability.
A variance over 25% is less desirable.
The higher the variance, the less reliable the forecast.
In this example, the variance of 8% is great.

![Forecast with Data](images/ForecastResults.004.png)

Using actual throughput history, our 18 week projection is showing a 10 to 35% probability.
20 weeks has a 40 to 75% probability.
22 weeks has a 80 to 95% probability and 26 weeks is 100% probable.

You will find varying outcomes, depending on your actual throughput history.
It isn't always worse than the range.
Sometimes it is better.
Sometimes it's the same.
But using the actual history always provides a more realistic simulation than using a range, so if you have the actual data, why not use it?

### Split Rate
Now, let's consider that the defined scope of this project has been growing.
This is also natural.
We've discovered new desirable features as we've gone along and we've been progressively elaborating which naturally results in some expansion of the scope size.

We started with a population of stories that represented 34 work units.
Looking at the work targeted, it has grown to 40 work units.
That's a split rate (or growth) of just over 17.5%.

So, let's use 18% as an approximation for our split rate and run it one more time.

![Forecast with Split Rate](images/ForecastSettings.003.png)

![Split Rate Forecast Results](images/ForecastResults.003.png)

This time, our split rate is 1.00 for the low, indicating no split, and 1.18 for the high, indicating that for every 50 units we start with, we end up with 59 units in final scope.

Now, our probability of completing in 18 weeks had dropped to a range of 5 to 10% and a much more likely completion is 24 weeks from now, with a 95% probability.

#### Determining Split Rate

So far, we've hand waived over split rate.
Now, let's dive into it a bit and see how we can determine a reasonable split rate range for our project.

A split is anything that causes a single item to become more than one item.

In some cases, this is an actual story split.
In iteration planning, for example, we realize that we can actually split this story into smaller deliverable pieces.

In some organizations, this type of split has to follow a set of rules that indicate a story must always split into smaller stories that total the original estimate.
In other organizations, a story may split into pieces that exceed the original estimate.

It is ultimately up to you to figure out how you want to handle this in your own place of work, but if you are asking my advice, split the story and estimate the work as it is now articulated.
Don't worry about the original estimate/size.
Forcing teams to stick to an original estimate when they discover a new way of executing on the work is an arbitrary rule that offers no additional value and is practically guaranteed to result in more consternation and waste.

A second example of a "split" is an actual growth in scope.
Say we deliver a feature and then learn that our customers want it enhanced in some way.
This is essentially the same as a split.
We had a single work item, which, when delivered, produced yet another work item.

Another possible example of a split is discovery of a defect.
If you record defects in your throughput, then any time you discover a defect, it is essentially a split of the work item that generated the defect.
You had one work item.
You delivered it.
You now have two work items related to the same customer value; the original work item and the defect.
In some organizations, throughput is based on delivery of value work.
Defects don't count in these organizations.
In others, all work is counted.
I've waffled back and forth on this over the years and have pretty much landed in the, "it's up to you" camp.
Whatever you do, do it consistently.

As we can see, split rate is a bit involved and may be hard to measure.
I tend to start teams with a split rate of 1.5 to 2.0, indicating that for every one work item in the backlog, we will have 1.5 or 2.0 work items at completion.
When introducing the concept, I'll ask a team to grab 10 items out of the backlog, take a look at them and give me ball park on how many work items will result from these 10; including or excluding defects in accord with their policy on defect sizing/tracking.

To measure actual split rate is a bit difficult, but not impossible.
Let's say we are targeting to release a specific feature.
Once a week or so, we record the total number of work items (or points) that need to be completed in order to deliver the feature, including the work already completed.
Let's say that at the beginning, we have 100 units to complete.
By the time the feature is delivered, there are 175 units completed.
We now know that the split rate is 1.75 - for every work unit we began with, we ended up with 1.75 work units.

It is important to measure the work completed.
Otherwise, you may over estimate the split rate.

Say, for example, you discover 10 defects, but 9 of them are low priority and will not need to be addressed in this feature release.
Then only one of those defects counts toward the split rate.
Similarly, if you identify new work that doesn't get prioritized into the existing release (or may never get done), then it doesn't count in the split rate.

### Conclusion

Using this forecasting technique has provided some very tangible benefits.

The first is the improved accuracy of the forecasts.
Using this technique, we get a much better understanding of the probability that we can hit a date.
We get a better sense of when the work will be completed.

The second is the quality of the conversations as a result of using this technique.
Teams are able to talks about probability and help make informed decisions about delivery.
I've had numerous instances with this technique where, given dates and probabilities, we were able to move deadlines to a higher probability date.
I've also had a few instances where we were able to clearly identify scope to reduce in order to meet an immovable date.

In the past, we believed that in order to be able to reliably forecast, we had to first stabilize our velocity.
This was because we were using standard velocity math.
Velocity math was hard to believe when we could _see_ the data was highly variable.
As it turns out, we should have generally been suspicious of velocity math.

Using the Monte Carlo Simulation technique, while a stable throughput is certainly beneficial, it is not necessary.


[^Forecaster]: Magennis, T. (n.d.). Throughput Forecaster. Retrieved February 23, 2018, from https://github.com/FocusedObjective/FocusedObjective.Resources/raw/master/Spreadsheets/Throughput Forecaster.xlsx

[^split-rate]: Magennis, T. (n.d.). Calculating Work Split Rate â€“ Ground-Speed vs Air-Speed. Retrieved March 28, 2018, from http://focusedobjective.com/calculating-work-split-rate/
