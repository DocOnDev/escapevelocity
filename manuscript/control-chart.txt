{#cycle-time-control-chart}
### Cycle-Time Control Charts

So we've taken a look at Cycle Time for the varying stages of our process and we've determined the issue is not necessarily with development.
It appears there is more opportunity for improvement in testing and deployment.

Let's say testing, on average, takes slightly more than 12 days.
We can take a look at all testing work and try to find ways to optimize it, but maybe there is a better way to look at the data.

A cycle time control chart can help us find outliers in our flow.
By focusing in on these outliers, we can likely learn and improve more rapidly.

The cycle time control chart shows the cycle time for each story over a period of time.
For each story that completes testing and moves into approval, we note the date it moved and the cycle time for testing.

Just like our lead times, cycle times occur on a right-skewed distribution.
To be more specific, lead times and cycle times for agile software projects tend to fall on a Weibull distribution with a shape parameter between 1.3 and 1.6.

For a normal distribution, commonly referred to as a bell curve, we can apply standard deviation to help us focus in on the outliers.
For a normal distribution, a single order standard deviation represents the 68.3rd percentile, a second order standard deviation represents the 95.4th percentile, and a third order represents the 99.7th percentile.
Unfortunately, standard deviation does not apply correctly to skewed distributions.
So we instead need to calculate a computed percentile[^ComputedPercentile].

A scatter diagram with lines for the 50th and 95.4th percentile gives us a basic cycle time control chart.
The 50th percentile represents the median and the 95.4th percentile in this case approximates a second order standard deviation.

![Cycle Time Control for Testing](images/CFD.010.jpeg)

We can see from our example chart, that the median cycle time in testing for these stories is 12.4 days, which is represented by the gray bar.
Our 95.4th percentile is at 23.2 days, which is represented by the red bar.
While we want to drive the average cycle time down, it might be good to start with the outliers that are above the red bar.
Not only do these push the average up, but they widen the "normal" range due to a larger deviation.

Focusing on those items above the red line, we can look for clues as to why they took so long, create a hypothesis about the cause, and devise an experiment to run.
Maybe we determine that all of the high outliers involve a specific area of the code.
Or perhaps they all involve the use of a specific technology or a specific workflow within the application.
And, of course, there may be more than one reason for the issues.
By focusing on the outliers, we narrow the possible causes and can be more confident that we are addressing the highest impact issues first.

Let's say all but one of these involve the use of data that takes 11 days on average to refresh from production.
The team confers and determines that the data can be scheduled to refresh every 30 days and as long as the data has been updated in the last 90 days, it is sufficient for testing.
The one significant outlier was due to testing of integration with a third party application.
The team did not request a QA server for the application until the story was picked up for testing.
They've added a simple check to their iteration planning meeting.
For any stories that require integration testing, they now confirm a testing environment exists, and if not, make the request right away.

The next several weeks now look like the following graph:

![Cycle Time Control for Testing after Optimizations](images/CFD.011.jpeg)

The average (mean) cycle time in testing for these stories is 10.3 days and the 95.4th percentile has tightened from 23.2 days to 15.7 days, creating a new small set of outliers.
The team can now focus in on these outliers, identify root causes, and start a new set of optimization experiments.

In addition to the outliers, you may want to pay some attention to trending and other indicators.
We're not going to cover more detailed statistical process control in this book, but you may want to take a look at Nelson Rules[^NelsonRules] or Western Electric Rules[^WesternElectricRules] for identifying bias, trending, and noise.
As an example, according to Nelson Rules, any time you have 14 or more points that oscillate above and below the mean, you have significant noise.
This is something worth digging into even when none of the points are significant outliers.
This much oscillation indicates a problem in the system.

[^ComputedPercentile]: Magennis, Troy. "The Economic Impact of Software Development Process Choice Cycle Time Analysis and Monte Carlo Simulation Results." June 19, 2015. Accessed December 14, 2018. https://github.com/FocusedObjective/FocusedObjective.Resources/raw/master/Presentations/The-Economic-Impact-of-Software-Development-Process-Choice-Cycle-time-Analysis-and-Monte-Carlo-Simulation-Results.pdf.iÂ 

[^NelsonRules]: "Nelson Rules." Wikipedia. September 28, 2018. Accessed Jan 16, 2019. https://en.wikipedia.org/wiki/Nelson_rules.

[^WesternElectricRules]: "Nelson Rules." Wikipedia. September 28, 2018. Accessed Jan 17, 2019. https://en.wikipedia.org/wiki/Nelson_rules.
