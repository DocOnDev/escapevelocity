# Balanced Metrics

We've discussed a number of possible metrics to augment or even replace velocity.
Rather than looking for the one true measure, it is important that we create a
portfolio of measures that properly inform us about our process and progress.
My advice to teams is to measure many things.
Or, in the words of David J. Bland, "Measure what you want to go up, but also measure
what you don't want to go down."

{icon=quote-left}
G> Measure what you want to go up, but also measure what you don't want to go down.
G> https://twitter.com/davidjbland/status/856643503606030336

We know that measuring and making the measurement known changes behavior and thereby the outcomes.
We know this from [Hawthorne Effect](#hawthorne-effect).
So even if we don't set targets, which would invoke [Goodhart's Law](#goodharts-law), we still have a challenge in that measuring lead time might result in lower lead times, but we might sacrifice quality to do so.
What are we to do?

Maybe we could measure many things.
What if we anticipated that measuring lead time would possibly lead to lower code quality?
The team moves faster by compromising on discipline.
Well, we'd measure code quality, of course.

And what if we thought that maybe measuring quality and lead time would result in delivery of less value where the stories move quickly and are defect free, but don't meet customer needs?
Then why not also measure feature use, revenue generated, or customer satisfaction?

We've covered a number of metrics in this book, but our list is by no means comprehensive.
Think about the system you are in and the standards you want to adhere to.
How would you objectively know you are meeting those goals?
How would you know if you are trending in the right direction in pursuit of those goals?
Establish a set of measures you can take that inform you and pay attention to them.

If you find a particular measure is not particularly informative or useful in pursuit of your goals, stop measuring it.
As an example, at one client, we stopped performing customer surveys and started measuring interactions and conversions.
Interestingly enough, we found it was relatively common for customers to give changes a low rating while simultaneously increasing their interactions and conversions.
Our goal was conversions - enticing customers to take certain actions that lead to revenue for us.
I'm not saying we didn't want customers to be happy with our software.
What I am saying is customers often reported mediocre opinions of a change while simultaneously engaging more with the software in an increased revenue positive way.
The data from actual usage was move accurate than the opinions of the customers who took the time to complete the surveys.

This won't always be the case.
I'm not advocating that companies don't survey customers.
I'm advocating that you consider what you are trying to achieve and determine which measurements best inform you toward that end.

## A starting point ##
I am often asked what metrics a team (or teams) should use.
This question often comes devoid of much detail, such as the make up of the team, how long they've been together, what pain they are experiencing, or what objectives they're trying to measure for.
In these cases, I tend to advise teams to start with four basic metrics - [feature use](#feature-use), [lead time](#lead-time), [code quality](#code-quality), and [team joy](#team-joy).
These four metrics allow a team to determine if the customer is using the software, if their time to delivery is improving, if they are managing consistent or improving quality, and if the team feels good about the work.
With this information, they can tweak their efforts and find that sweet spot where they are moving as fast as possible without sacrificing quality and not burning out the team all while adding value for the customer.

I am not saying these are the only measures you should use.
I am not saying these measures are mandatory.
What I am saying is, when I'm asked where a team should start without sufficient context or experience, this is where I tell them to start.
It is a short, decent set of metrics that will inform them.
From there, they can adjust.
