## Demand for Higher Velocity

This is far and away, the most common Velocity Anti-Pattern, and quite possibly the most harmful.
It manifests itself in a number of differing fashions, but the basics are the same:
Somebody determines that the team needs to get more done in less time.
So they send out the message - "We are going to need more velocities."
This person is usually an authority figure and typically doesn't do the actual work being asked of the team.
And they clearly don't know what velocity is.
More velocities? Aw C'mon, really?

### Why we need "more velocities"

In some cases, the need for higher velocity is based on a set scope for a set date and some basic math with significantly flawed assumptions.
Take the total work to be done and divide it by the team's average velocity.
If the number of weeks to complete the work exceed the number of weeks between now and the deadline, then the team needs to get more done in less time.
All of this typically under-represents the volatility of the velocity, fails to take into account systemic issues, is based on estimates made with very little information, and assumes a known and locked scope.

In other cases, a potential for higher velocity is observed, which then creates a demand for higher velocity.
These observations are steeped in a lack of true understanding of creative work and an adherence to a Tayloristic[^Taylorism] output-centric management style.
The team can be seen "loafing".
There are times where nobody is actively writing code.
There are times where more than one individual is working on the same basic piece of work at a given time.
Sometimes, we can even see two or more people working at the same computer at the same time.
Not everybody punches in at 8am and out again at 6pm.
Some people don't eat lunch at their desks.
And who knows what people are actually doing when they "work from home?"
This team can clearly move faster - we just need to give them a little push.

And in other cases, the need for higher velocity is borne of expectation.
Everybody knows that when a team goes agile, they get faster.
It's a fact.
It has been written in numerous agile books, especially books on scrum.
I mean, I just wrote it in this book.
So it must be true.
And if it is true that teams get faster, all we need to do is help them get there.

These are but a few of any number of reasons why we might expect or "need" the team to move faster.
This thinking is, unsurprisingly, flawed.
Velocity is not about measuring the team.
It is about having a coarse-grained forecast.
Rather than a tool to rate and push a team, it is a tool to help make key business decisions.
Which features can we cut back on?
What is truly priority?
How else might we organize the work, support the team, or think about the product?

Unfortunately, these are hard decisions.
They may force us to make tradeoffs.
They may result in our having to change our public message; to move a date or to reset market expectations.
It is far easier to defer the hard decision and ask a team to "step up".
It is easier to abdicate the responsibility and push it to the team.

{icon=quote-left}
G> Velocity isn't about measuring the team. It is about having a coarse-grained forecast.

In most cases where the need for an increase in velocity is articulated, there is an underlying unspoken premise.
The belief that if properly motivated, we can do better.
Re-phrased, the belief that we are not already operating at our best.

**Given** we need to hit a deadline with a set scope

**When** the team is not moving fast enough to hit the deadline

**Then** the team is capable of moving faster

This is logically flawed.
The conclusion is baseless and is in no way supported by the precedents.
Just because we *want* a team to move faster does not mean that they can.
And quite often, pushing a team to go faster ultimately slows them down.

[^Taylorism]: What is Taylorism? Retrieved April 12, 2017, - http://www.businessdictionary.com/definition/Taylorism.html

### Attempts to entice more velocities

Leaders (and teams) attempt to achieve velocity increases in numerous ways.
Most, as you can imagine, have unintended side effects on the teams and none significantly improve the actual flow/delivery of value to their customers.
Here we explore a few ways teams might be encouraged to increase their velocity.
Unfortunately, no matter how well intentioned, using such techniques still has a negative impact.

#### Increase Awareness

The number one motivational technique is simply to increase awareness.
We make the team acutely aware of the deadline, the desired scope, and how near or far they are to hitting the target.
This is a simple and passive technique.
The idea here is that if people know what is required, they will rise to the occasion.

Burn down charts and burn up charts are a common technique for showing the team how they are doing within an iteration or towards a release.
No one can be accused of pressuring a team by simply making data available, right?

But no matter how seemingly benign, this approach invokes the [Hawthorne Effect](#hawthorne-effect), which practically guarantees the measurement will improve, but does not guarantee the overall results will improve.

I am not saying that you should not use charts or make data available to the team.
What I am saying is that you need to be aware that the team will tend toward behavior that "improves" the measurements by which they feel they are being evaluated.

More often than not, velocity will appear to increase.
This is often the result of some increase in the average estimate of a story, resulting in higher velocity numbers but no measurable increase in value.
The team begins to wonder if they're estimating correctly.
Maybe that 3 is really a 5; we're not entirely positive how complex it is.
Better to be safe than sorry.
We don't want an artificially low velocity - you know, like we used to have.

Measurements should be data the team considers valuable and wishes to see rather than data that management wants for evaluation or demands for process consistency.
If management is going to mandate certain measurements, educate the team on what the measurements are and how they serve the team and the organization.
Finally - if management is going to mandate certain measurements, then make sure the mandated measurements are well [balanced](#balanced-metrics).


#### Velocity Goals

So we've shown the team how they are doing.
We gave them the burn charts and they still didn't go fast enough.
What is a good ScrumMaster to do?
Well, help them with the math, of course.

"Hey team. We can see that our burn chart is looking less than ideal.
If we want to hit our deadline - and we *do* want to hit our deadline - it looks like we're going to need to increase our average velocity from 22 to 26 as soon as you can.
Whaddya say? Can we count on you to work smarter, not harder?"

This, of course, isn't the only way targets are set.
Some managers take a more dictatorial approach.

"Okay team.
I didn't want to have to do this, but you people can't seem to figure it out on your own.
So much for 'self organizing'.
Your velocity sucks.
I've looked at your burn chart and you can't possibly make it.
Effective immediately, velocity must increase to 26 points.
I don't want to hear any whining.
You've been lallygagging long enough.
Now get back to work."

The language is different, but the intent is the same and the end result is likely the same.
Velocity will very likely go up.
And again, this does not mean the end result will improve.

In fact, what happens is that we've now invoked [Goodhart's Law](#goodharts-law).
In setting a target for a lagging indicator in an attempt to control the system, what has actually happened is we've changed the system, thereby changing what the indicator means.
As a result, the measurement is no longer the same as the measurement we were using before and the target doesn't mean what the manager thinks it means.

#### Rewards
Rewards for increased velocity fall into a category of their own.
Here, we must have some level of awareness.
After all, we can't assess an increase in velocity if we have no measure.
And we additionally have a target.
If we're offering a reward for an increase in velocity, there must be a target or even set of targets to which awards are tied.
So we've invoked both [Hawthorne Effect](#hawthorne-effect) and [Goodhart's Law](#goodharts-law).

As we know from prior discussion, this means both that the velocity will increase artificially (or temporarily) and that the velocity measurement will not mean what it once did.
But it gets worse.

You see, in knowledge work, financial rewards actually impede performance.
That's right.
Financial incentives impede performance.

In 1971, Mark Lepper and colleagues at Stanford ran a study wherein they invited students to work on puzzles and games[^Overjustification].
After a period of time, they started to reward students for their performance.
There was a pronounced improvement in performance among some of the students.
Then, the rewards were taken away and the students who had shown improvement regressed to levels below their original capability.
Some of them stopped participating altogether.

At first glance, one might conclude that the incentives improved individual performance and the problem was not their use, but the fact that they were discontinued.
But this is not the case.
You see, while some people's performance went up temporarily, the overall performance of the group sagged when incentives were introduced and only worsened when they were then retracted.
Extrinsic reward structures for work people found intrinsically motivating made performance worse.

Numerous other studies[^CandleProblem], including those spearheaded by University of Rochester psychologists Edward Deci and Richard Ryan[^SelfDetermination] have shown that rewards often undermine our intrinsic motivation to work on interesting, challenging tasks - especially when they are announced in advance or delivered in a controlling manner.

For those select few who actually display an increase in performance on cognitive tasks when offered extrinsic rewards, the reward levels tend to lose impact over time.
Whatever incentives are offered eventually become the "new normal" and maintenance of existing performance levels will require increases in reward.
This is not a sustainable model.

[^Overjustification]: Lepper, Greene, Nisbett on the "Overjustification" Hypothesis Retrieved April 15, 2017, - http://courses.umass.edu/psyc360/lepper%20greene%20nisbett.pdf

[^SelfDetermination]: Self-determination theory. (2017, April 07). Retrieved April 15, 2017, from https://en.wikipedia.org/wiki/Self-determination_theory

[^CandleProblem]: The influence of strength of drive on functional fixedness and perceptual recognition. Journal of Experimental Psychology, Vol 63(1), Jan 1962, 36-41. Retrieved April 15, 2017, from http://dx.doi.org/10.1037/h0044683

##### Velocity Shaming

While really just a variant on rewards, Velocity Shaming gets a special subsection all to itself as it reveals a more serious leadership shortcoming.

The term reward suggests some form of positive reinforcement.
With velocity shaming, we forego the positive reinforcement and instead shame or cavil[^cavil] the team for failure to meet expectations.
If the caviler is a seagull manager who swoops in occasionally and craps on the team, then shaming will occur perhaps a few times per quarter and will be about failing to average sufficient velocity.
If, on the other hand, the caviler is a micro-manager, the shame-fest will probably occur several times within a single iteration for failure to track to a velocity goal.

The more they pay attention, the shorter the shame cycle.
The shorter the shame cycle, the more harm done.

There is holding a team accountable and there is just plain bad leadership.
For most teams, the inability to move quickly is systemic.
From cross team dependencies and arduous process to insufficient support, staffing, and funding, teams are often burdened with herculean tasks that their leadership could make easier.
Shaming a team for their inability to function to your liking in a system which you as management support through inaction is a special kind of awful.

If you are in charge of a team and you do this; and you know who you are.
Stop it.

Stop it now.


[^cavil]: To raise irritating and trivial objections; find fault with unnecessarily - Cavil. (n.d.). Retrieved March 19, 2018, from http://www.dictionary.com/browse/caviler

### Attempts to show more velocities

When leadership asks for an increase in velocity, there are a few common behaviors that occur.
Each of them are an attempt to satisfy the potentially unrealistic ask.

It is intriguing to me how often a manager will make a change such as this to a system of work and then later proclaim that the team is gaming the system.
This is simply not the case.
In fact, the gaming of the system is the improper application of targets or goals for lagging indicators.
The rest is just natural consequence.

The following are but a few examples of what happens when a manager games the system.

#### Inflating Points

A few years back, I was working with an organization where teams were achieving the expected increase in velocity, but leadership didn't feel like things were moving any faster.
Selecting one team, we looked at their history and found that their velocity had gone from an average of about 20 points to an average of about 40.
On a hunch, we ran a report to figure out the average story size for each iteration.
Sure enough, the average story size had gone from around 1.4 to around 3.1.
In fact, you could see a punctuated increase in average size followed by a slow but steady incline.
The punctuated increase came right around the time leadership introduced the new burn up charts and put a focus on velocity.

Looking at the average number of stories completed per iteration, the numbers were telling.
With an average velocity of 20 at 1.4 points per story, they were completing around 14 stories per iteration.
With an average velocity of 40 at 3.1 points per story, they were completing around 13 stories per iteration.

Were they in fact moving faster and the stories were coincidentally larger?
How could we know for sure?

We took an evenly distributed sampling of stories across the history of the project and printed them out without any sizing information.
We then asked the team to size the stories using the same techniques they'd always used.
The average story size came out to be approximately 2.8 with earlier stories growing from an average of 1.4 to 2.6.

Had they been gaming the system?
No. As we've already discussed; we game the system when we change it, the resulting behaviors are just natural consequence.
We can't know for sure if there was, at one time, a deliberate increase in story sizes, but we can say that under the given conditions, the team genuinely believed the larger numbers were more accurate.

#### Splitting Points

Splitting points refers to two possible activities; taking partial credit for work picked up in a given iteration, but not completed or breaking stories up into smaller pieces that add up to more than the original.

##### Taking Partial Credit

This isn't really a way of making our velocity look greater, but it is a way of making our velocity look "better", so I've added it here.

Say we have an 8 point story that we pick up mid-iteration and don't complete.
At the end of the iteration, we award ourselves 3 points and roll 5 points into the next iteration.
The intent, I believe, is to be able to represent the effort expended in each iteration.
This allows for a smoother looking burn-down chart across a release.
And that means less people are asking questions.

Of course, if we're not actually delivering value in a given iteration, we probably shouldn't be taking partial credit based on "effort".
And if this is happening consistently, maybe those people should be asking questions.

It is not uncommon for teams that engage in this practice to do it repeatedly.
I've seen plenty of teams that split and roll.
At the end of an iteration, they split all stories in process and roll the remainders into the next iteration.
In some cases, such as when they use an electronic tracking system, they  actually create a duplicate card, split the points, and move the new card into the next iteration.

I've also seen this happen across multiple iterations.
An 8 point card gets split into 3 done and 5 remaining.
That 5 gets split into 2 done and 3 remaining which gets split into 2 and 1.
Four iterations to complete an 8 point story.
And we've lost all visibility of the lead time and cycle times on that story.

Burn charts look good.
Velocity seems pretty consistent.
Bonuses will be paid to the ScrumMasters.
And yet, we've no idea when the work in this iteration will actually be done.
We're foolish enough to proclaim that we consistently get 12 points done per iteration, so this 5 point story should easily get done next iteration.
We're ignoring the fact that the work completed in a given iteration was actually started in prior iterations and that odds are very low we'll ever finish 5 points on a card in a single iteration.

##### Breaking Up Stories

This one is interesting because it is often done for the wrong reasons, but sometimes it works.

Some teams will take a large story and break it into smaller pieces in order to make it look bigger.
Take a team that estimates on a Fibonacci scale where stories are sized as 0, 1, 2, 3, 5, 8, 13, 21, and so on.
A 5 point story gets broken into three pieces, each estimated at 2 points.
Viola, 5 points becomes 6.
Nice.
They couldn't convince management it was an 8 (thanks Fibonacci), but through some slight of hand, they bumped it to 6.
More Velocities, here we come.

Seems a tad dubious, does it not?

But it can actually work.
And I don't mean it can work because it artificially increases your velocity, which it does.
I mean it can work because you get more work done in less time.

Assume that stories are split along reasonable seams into chunks of work that can be individually delivered.
Now the team has increased the likelihood they can both deliver value in this iteration and improve their throughput.

In some environments I've coached, we've seen an interesting outcome when stories get broken down into smaller slices.
We found that when we broke large stories into discrete parts, the average time to deliver all discrete parts was less than the time to deliver the larger story.
This held true even when the parts added up to more than the story.
For more on this, you can read the section on [correlations](#correlations), we discuss [Lead Time by Story Size](#lead-time-story-size).

If you're going to split stories, do it for the right reasons.
Do it because you want to optimize for learning as you go.
Do it because you want improved forecasting.
Do it because you want a more consistent delivery cadence.
Do it because you want to get working software into the hands of your customers as soon as possible.

And a side effect will be that your actual throughput goes up.
