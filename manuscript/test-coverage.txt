### Test Coverage

Test Coverage isn't really a measure of quality.
Test coverage is nothing but a measure of how much of the code is covered by
tests.
I've seen plenty of teams with high test coverage achieved through some form
of integration test with awful internal code quality.
I've seen teams with extremely low coverage with good, if not excellent, internal
code quality.

So why would I advocate for test coverage as a metric and why would I further
classify it as a quality metric?
I mean I just said it isn't really a measure of quality.
That's true.
Test coverage is not a measure of quality.
But having test coverage is an indicator of quality.

Allow me to explain.
Code that is poorly written is hard to test in isolation.
And code that is hard to test in isolation is poorly written.
To test in isolation is to test a single piece of functionality, such
as a method or function, without exercising code in other methods or functions.
The more dependencies the code has, the more complex it is, and the move convoluted the
internals, then the harder it is to test in isolation.
The inverse is also true; the easier code is to test in isolation, then the less convoluted
it is, the more simple it is, and the fewer dependencies it has.

If I cannot test a method without also testing another method, then the two
methods are said to be coupled together.
When methods are coupled together such that I cannot test one method without testing
both methods, then there is a good chance I cannot change the first method
without changing both methods.

If we find this kind of coupling once or twice in a code base, it might be of
little concern.
But the more often it happens, the harder it is to extend or update the code.
The application gets brittle; seemingly trivial changes have cascading impact.
One small change can break several things, some of which seem un-related.
Corrections to those breakages result in yet more cascades.

The more often a "simple" change takes far longer than one might expect, the
higher the chances are you have a poorly constructed code base.

Unit tests, tests that validate a method's behavior without calling any other
methods, are only possible when the code is constructed in a way that indicates
loose dependencies or loose coupling.
The amount of code successfully covered by unit tests, therefore, is an indication
of how well the code is written.

#### Don't set a target

A common mistake I see is teams who get excited about test coverage and set a
goal for it.
once they can see how test coverage negatively correlates to bugs and cycle
times, they want more coverage in order to reduce defects and to deliver faster.
The logic is usually that if 30% test coverage is good, then 50% must be better
and 100% would be better still.

Don't do this.

100% coverage indicates tests over all possible forks and branches of the code.
It suggests that all edge cases and exceptions have been tested.
While this sounds good in theory, when we make coverage the goal rather than
good testing the goal (which is harder to quantify), we might miss defects even
though we hit our target coverage number.

That's right; 100% coverage doesn't mean you'll find all the bugs.

Let's take a look at a simple piece of code with two if statements in it.

{title="Less Than Four", lang=ruby}
~~~~~~
# Always return a number less than 4
def less_than_four(first_check, second_check)
  result = 0
  if first_check
    result+=1
  else
    result+=2
  end

  if second_check
    result+=2
  else
    result+=1
  end

  return result
end
~~~~~~

Now, we cover this code with a couple of tests which exercise both the "if" and
the "else" for each of the checks.

{title="Tests", lang=ruby}
~~~~~~
expect(less_than_four(true,true)).to be < 4
expect(less_than_four(false,false)).to be < 4
~~~~~~

This gets us to 100% test coverage.
But in this case, 100% coverage is not enough.
A test for (false, true), returns a result equal to 4, which is a bug per
the intended behavior of the function.
100% coverage means we've passed through each line of code at least once in our
testing, it does not mean we've exercised all combinations and permutations of
the code.

You might be thinking, "Okay, Doc. But isn't that an argument for 100% test coverage
as a minimum target?"

Perhaps.
In this case, I am saying 100% test coverage isn't sufficient to indicate we're
bug free.
I am saying that our test coverage needs to be thought through, not just based on
a minimum target.

In some cases, like the one described above, 100% test coverage is simply not enough.
And in some cases, 100% test coverage is simply too much.

Let's take a common scenario where a team is working on a long-established code
base.
The product has been around for three or four years, consists of hundreds of
thousands of lines of code and has no automated tests to speak of.
I see this in a great number of organizations that rely on manual testing or
organizations that tried to use some form of record/playback test automation
until it got too hard to maintain and abandoned the effort.
In either case, they've now got 0% test coverage.

What is this team to do?
Do they try to get all the code covered with tests?
If so, do they stop feature development until they have 100% coverage?
Or maybe they set a target of 100% and try to both deliver new features and
increase coverage.

I say they do none of these things.
Instead, concentrate on writing tests around the code you're working in.
Don't bother with anything else.
If you need to change existing code, write enough tests to feel safe making the
change.
If you are writing new features or functions, get comprehensive testing around
them.
Make sure anything you modify or add is well tested.
Don't worry about the rest.
Code that has been in production, has no existing tests, has no reported defects,
and doesn't need to change doesn't need test coverage at the moment.
When there is a defect reported or the functionality needs to change, then you
need tests around it.

Following this technique, you are addressing the highest risk - the code with
the highest churn rate.
The term churn indicates the rate of change.
Code that changes often is said to have a higher churn rate than code that changes
infrequently or not at all.
It may turn out that numbers as low as 30% overall test coverage are sufficient
to safely work on the code base and help to ensure no new escaped defects.
Focusing on 100% coverage for the entire code base would result in a lot of work
with very little return.
In these cases - and my experience is that these are the common cases - 100%
could reasonably be considered too much test coverage.

So if 100% test coverage may be insufficient and 100% test coverage may be too
much, you can see why I don't advocate for test coverage targets.
No single number universally indicates quality or safety.
Test coverage needs to be achieved intentionally; thinking about what needs to
be tested and why and how.

If we don't set a target, why have the metric at all?

Interesting question.
While no one number is a clear indication of sufficient or insufficient test
coverage, test coverage is negatively correlated to risk.
An increasing percentage of test coverage indicates a diminishing risk, a
decreasing percentage of test coverage indicates a growing risk, and a steady
percentage of test coverage indicates a steady level of risk.
This is a generalization.
You can't count on test coverage alone as an assessment of risk.
Test coverage won't tell you about the market, how brilliant your idea is, or
whether or not you've sufficient funds to keep operating.
But in monitoring the trend of test coverage around the code with high churn,
you can get a sense of risk around changes to the code base.
